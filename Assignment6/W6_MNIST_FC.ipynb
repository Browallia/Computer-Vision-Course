{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28])\n",
      "torch.Size([60000, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/browallia/anaconda3/lib/python3.8/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n",
      "/home/browallia/anaconda3/lib/python3.8/site-packages/torchvision/datasets/mnist.py:62: UserWarning: train_data has been renamed data\n",
      "  warnings.warn(\"train_data has been renamed data\")\n",
      "/home/browallia/anaconda3/lib/python3.8/site-packages/torchvision/datasets/mnist.py:52: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n",
      "/home/browallia/anaconda3/lib/python3.8/site-packages/torchvision/datasets/mnist.py:67: UserWarning: test_data has been renamed data\n",
      "  warnings.warn(\"test_data has been renamed data\")\n",
      "/home/browallia/anaconda3/lib/python3.8/site-packages/torchvision/datasets/mnist.py:57: UserWarning: test_labels has been renamed targets\n",
      "  warnings.warn(\"test_labels has been renamed targets\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# torch.manual_seed(1)\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "EPOCH = 10\n",
    "LR = 0.001\n",
    "DOWNLOAD_MNIST = True\n",
    "\n",
    "train_data = torchvision.datasets.MNIST(root='./mnist/', train=True, transform=torchvision.transforms.ToTensor(),\n",
    "                                        download=DOWNLOAD_MNIST, )\n",
    "test_data = torchvision.datasets.MNIST(root='./mnist/', train=False)\n",
    "print(train_data.train_data.shape)\n",
    "\n",
    "train_x = torch.unsqueeze(train_data.train_data, dim=1).type(torch.FloatTensor) / 255.\n",
    "train_y = train_data.train_labels\n",
    "print(train_x.shape)\n",
    "\n",
    "test_x = torch.unsqueeze(test_data.test_data, dim=1).type(torch.FloatTensor)[:2000] / 255.  # Tensor on GPU\n",
    "test_y = test_data.test_labels[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mnist(model, batch_size, epochs, LR, weight_decay=None, lr_schedular=None):\n",
    "    \"\"\"train pipeline\n",
    "    \"\"\"\n",
    "    if weight_decay is not None:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=weight_decay)\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "    if lr_schedular is not None:\n",
    "        schedular = torch.optim.lr_scheduler.StepLR(optimizer, epochs//3, 0.1)\n",
    "\n",
    "    data_size = 20000\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    print(f'-----model: {model.get_name()}, batch_size: {batch_size}, epochs: {epochs}, LR: {LR}, weight_decay: {weight_decay}, lr_schedular: {lr_schedular}-----')\n",
    "    for epoch in range(epochs):\n",
    "        random_indx = np.random.permutation(data_size)\n",
    "        for batch_i in range(data_size // batch_size):\n",
    "            indx = random_indx[batch_i * batch_size:(batch_i + 1) * batch_size]\n",
    "\n",
    "            b_x = train_x[indx, :].cuda()\n",
    "            b_y = train_y[indx].cuda()\n",
    "    #         print(b_x.shape)\n",
    "    #         print(b_y.shape)\n",
    "\n",
    "            output = model(b_x)\n",
    "            loss = loss_func(output, b_y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if batch_i % int((data_size // batch_size)/3) == 0:\n",
    "                test_output = model(test_x.cuda()).cpu()\n",
    "                pred_y = torch.max(test_output, 1)[1].data.squeeze()\n",
    "                # pred_y = torch.max(test_output, 1)[1].data.squeeze()\n",
    "                accuracy = torch.sum(pred_y == test_y).type(torch.FloatTensor) / test_y.size(0)\n",
    "                print('Epoch: ', epoch, '| train loss: %.4f' % loss.data.cpu().numpy(), '| test accuracy: %.3f' % accuracy)\n",
    "        if lr_schedular is not None:\n",
    "            schedular.step()\n",
    "\n",
    "    model.cpu()\n",
    "    test_output = model(test_x[:10])\n",
    "    pred_y = torch.max(test_output, 1)[1].data.squeeze()  # move the computation in GPU\n",
    "\n",
    "    print(pred_y, 'prediction number')\n",
    "    print(test_y[:10], 'real number')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC(nn.Module):\n",
    "    def __init__(self, dropout=False):\n",
    "        super(FC, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "        self.fc3 = nn.Linear(10, 10)\n",
    "        self.dropout = dropout\n",
    "        if self.dropout:\n",
    "            self.drop = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        if self.dropout:\n",
    "            x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "\n",
    "        output = x\n",
    "        return output\n",
    "    \n",
    "    def get_name(self):\n",
    "        return self.__class__.__name__\n",
    "\n",
    "class BasicBlcok(nn.Module):\n",
    "    def __init__(self, planes, residual=False):\n",
    "        super(BasicBlcok, self).__init__()\n",
    "        self.residual = residual\n",
    "        self.conv1 = nn.Conv2d(planes, planes, kernel_size=3, padding=1, stride=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1, stride=2, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        if self.residual:\n",
    "            self.res = nn.Sequential(\n",
    "                            nn.Conv2d(planes, planes, kernel_size=1, stride=2, padding=0, bias=False),\n",
    "                            nn.BatchNorm2d(planes)\n",
    "                        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        if self.residual:\n",
    "            res = self.res(res)\n",
    "            x += res\n",
    "        x = self.relu2(x)\n",
    "\n",
    "\n",
    "        return x\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, in_planes, planes, residual=False):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, padding=1, stride=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.layer1 = BasicBlcok(planes, residual)\n",
    "        self.layer2 = BasicBlcok(planes, residual)\n",
    "        self.pooling = nn.AvgPool2d(7)\n",
    "        self.linear = nn.Linear(16, 10)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.pooling(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "    \n",
    "    def get_name(self):\n",
    "        return self.__class__.__name__\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----model: FC, batch_size: 50, epochs: 10, LR: 0.001, weight_decay: None, lr_schedular: None-----\n",
      "Epoch:  0 | train loss: 2.3371 | test accuracy: 0.147\n",
      "Epoch:  0 | train loss: 0.3800 | test accuracy: 0.833\n",
      "Epoch:  0 | train loss: 0.3469 | test accuracy: 0.882\n",
      "Epoch:  0 | train loss: 0.3624 | test accuracy: 0.893\n",
      "Epoch:  1 | train loss: 0.1704 | test accuracy: 0.892\n",
      "Epoch:  1 | train loss: 0.3743 | test accuracy: 0.908\n",
      "Epoch:  1 | train loss: 0.3379 | test accuracy: 0.917\n",
      "Epoch:  1 | train loss: 0.0912 | test accuracy: 0.921\n",
      "Epoch:  2 | train loss: 0.2151 | test accuracy: 0.920\n",
      "Epoch:  2 | train loss: 0.1553 | test accuracy: 0.914\n",
      "Epoch:  2 | train loss: 0.1415 | test accuracy: 0.931\n",
      "Epoch:  2 | train loss: 0.2040 | test accuracy: 0.932\n",
      "Epoch:  3 | train loss: 0.1391 | test accuracy: 0.932\n",
      "Epoch:  3 | train loss: 0.1761 | test accuracy: 0.938\n",
      "Epoch:  3 | train loss: 0.1428 | test accuracy: 0.939\n",
      "Epoch:  3 | train loss: 0.0934 | test accuracy: 0.941\n",
      "Epoch:  4 | train loss: 0.0294 | test accuracy: 0.941\n",
      "Epoch:  4 | train loss: 0.0348 | test accuracy: 0.950\n",
      "Epoch:  4 | train loss: 0.0771 | test accuracy: 0.942\n",
      "Epoch:  4 | train loss: 0.0294 | test accuracy: 0.946\n",
      "Epoch:  5 | train loss: 0.1806 | test accuracy: 0.946\n",
      "Epoch:  5 | train loss: 0.0347 | test accuracy: 0.944\n",
      "Epoch:  5 | train loss: 0.0255 | test accuracy: 0.947\n",
      "Epoch:  5 | train loss: 0.1041 | test accuracy: 0.956\n",
      "Epoch:  6 | train loss: 0.0199 | test accuracy: 0.957\n",
      "Epoch:  6 | train loss: 0.1259 | test accuracy: 0.955\n",
      "Epoch:  6 | train loss: 0.0069 | test accuracy: 0.957\n",
      "Epoch:  6 | train loss: 0.0843 | test accuracy: 0.956\n",
      "Epoch:  7 | train loss: 0.0141 | test accuracy: 0.956\n",
      "Epoch:  7 | train loss: 0.0530 | test accuracy: 0.955\n",
      "Epoch:  7 | train loss: 0.0708 | test accuracy: 0.955\n",
      "Epoch:  7 | train loss: 0.0287 | test accuracy: 0.958\n",
      "Epoch:  8 | train loss: 0.0263 | test accuracy: 0.959\n",
      "Epoch:  8 | train loss: 0.0130 | test accuracy: 0.954\n",
      "Epoch:  8 | train loss: 0.0183 | test accuracy: 0.956\n",
      "Epoch:  8 | train loss: 0.0528 | test accuracy: 0.961\n",
      "Epoch:  9 | train loss: 0.0364 | test accuracy: 0.962\n",
      "Epoch:  9 | train loss: 0.0130 | test accuracy: 0.963\n",
      "Epoch:  9 | train loss: 0.0133 | test accuracy: 0.964\n",
      "Epoch:  9 | train loss: 0.0254 | test accuracy: 0.962\n",
      "tensor([7, 2, 1, 0, 4, 1, 4, 9, 5, 9]) prediction number\n",
      "tensor([7, 2, 1, 0, 4, 1, 4, 9, 5, 9]) real number\n",
      "-----model: FC, batch_size: 50, epochs: 10, LR: 0.001, weight_decay: None, lr_schedular: None-----\n",
      "Epoch:  0 | train loss: 2.3302 | test accuracy: 0.106\n",
      "Epoch:  0 | train loss: 0.4422 | test accuracy: 0.779\n",
      "Epoch:  0 | train loss: 0.6683 | test accuracy: 0.827\n",
      "Epoch:  0 | train loss: 0.3596 | test accuracy: 0.867\n",
      "Epoch:  1 | train loss: 0.4722 | test accuracy: 0.868\n",
      "Epoch:  1 | train loss: 0.2149 | test accuracy: 0.882\n",
      "Epoch:  1 | train loss: 0.2095 | test accuracy: 0.887\n",
      "Epoch:  1 | train loss: 0.4819 | test accuracy: 0.906\n",
      "Epoch:  2 | train loss: 0.1828 | test accuracy: 0.897\n",
      "Epoch:  2 | train loss: 0.2935 | test accuracy: 0.914\n",
      "Epoch:  2 | train loss: 0.1691 | test accuracy: 0.913\n",
      "Epoch:  2 | train loss: 0.2795 | test accuracy: 0.916\n",
      "Epoch:  3 | train loss: 0.2619 | test accuracy: 0.923\n",
      "Epoch:  3 | train loss: 0.1886 | test accuracy: 0.920\n",
      "Epoch:  3 | train loss: 0.2250 | test accuracy: 0.919\n",
      "Epoch:  3 | train loss: 0.1877 | test accuracy: 0.922\n",
      "Epoch:  4 | train loss: 0.1460 | test accuracy: 0.919\n",
      "Epoch:  4 | train loss: 0.1183 | test accuracy: 0.924\n",
      "Epoch:  4 | train loss: 0.1640 | test accuracy: 0.924\n",
      "Epoch:  4 | train loss: 0.1288 | test accuracy: 0.933\n",
      "Epoch:  5 | train loss: 0.2737 | test accuracy: 0.930\n",
      "Epoch:  5 | train loss: 0.0688 | test accuracy: 0.928\n",
      "Epoch:  5 | train loss: 0.1758 | test accuracy: 0.934\n",
      "Epoch:  5 | train loss: 0.2316 | test accuracy: 0.924\n",
      "Epoch:  6 | train loss: 0.1450 | test accuracy: 0.929\n",
      "Epoch:  6 | train loss: 0.1356 | test accuracy: 0.939\n",
      "Epoch:  6 | train loss: 0.1686 | test accuracy: 0.933\n",
      "Epoch:  6 | train loss: 0.2331 | test accuracy: 0.932\n",
      "Epoch:  7 | train loss: 0.0926 | test accuracy: 0.936\n",
      "Epoch:  7 | train loss: 0.1022 | test accuracy: 0.936\n",
      "Epoch:  7 | train loss: 0.1257 | test accuracy: 0.928\n",
      "Epoch:  7 | train loss: 0.1687 | test accuracy: 0.942\n",
      "Epoch:  8 | train loss: 0.0812 | test accuracy: 0.942\n",
      "Epoch:  8 | train loss: 0.1606 | test accuracy: 0.938\n",
      "Epoch:  8 | train loss: 0.2743 | test accuracy: 0.936\n",
      "Epoch:  8 | train loss: 0.0721 | test accuracy: 0.945\n",
      "Epoch:  9 | train loss: 0.0560 | test accuracy: 0.945\n",
      "Epoch:  9 | train loss: 0.0350 | test accuracy: 0.938\n",
      "Epoch:  9 | train loss: 0.1788 | test accuracy: 0.936\n",
      "Epoch:  9 | train loss: 0.1662 | test accuracy: 0.938\n",
      "tensor([7, 2, 1, 0, 4, 1, 4, 9, 6, 9]) prediction number\n",
      "tensor([7, 2, 1, 0, 4, 1, 4, 9, 5, 9]) real number\n",
      "-----model: CNN, batch_size: 50, epochs: 10, LR: 0.001, weight_decay: None, lr_schedular: None-----\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 2.00 GiB total capacity; 1008.72 MiB already allocated; 0 bytes free; 1.02 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-b525bfed2b46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_mnist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfc_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLR\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrain_mnist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLR\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-f234e2897ccf>\u001b[0m in \u001b[0;36mtrain_mnist\u001b[0;34m(model, batch_size, epochs, LR, weight_decay, lr_schedular)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_i\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_size\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                 \u001b[0mtest_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m                 \u001b[0mpred_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0;31m# pred_y = torch.max(test_output, 1)[1].data.squeeze()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-4af80b9e918a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    437\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 439\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    440\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 2.00 GiB total capacity; 1008.72 MiB already allocated; 0 bytes free; 1.02 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "# experiment\n",
    "fc = FC().cuda()\n",
    "train_mnist(model=fc, batch_size=50, epochs=10, LR=0.001)\n",
    "fc_1 = FC(dropout=True).cuda()\n",
    "train_mnist(model=fc_1, batch_size=50, epochs=10, LR=0.001)\n",
    "cnn = CNN(1, 16).cuda()\n",
    "train_mnist(model=cnn, batch_size=50, epochs=10, LR=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7) prediction number\n",
      "tensor([7]) real number\n"
     ]
    }
   ],
   "source": [
    "test_output = fc(test_x[:1])\n",
    "pred_y = torch.max(test_output, 1)[1].data.squeeze()  # move the computation in GPU\n",
    "\n",
    "print(pred_y, 'prediction number')\n",
    "print(test_y[:1], 'real number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.3913, -6.0182,  4.3395,  4.6365, -6.4984, -2.3497, -5.2593, 11.5579,\n",
       "          0.0529,  4.0067]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 28, 28])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x[:1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f412e2bd9d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(test_x[:1].numpy().squeeze(), 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b9c185ecb12e0084425cdeecf3ea7a0d0aead0e7381acdc77ae5c3e2856d53f0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
